<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Atomic Language Machines</title>
  <link rel="shortcut icon" href="https://rednoise.org/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Noto+Sans:400,700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css" type="text/css" />
  <script src="https://code.jquery.com/jquery-3.4.1.min.js"
    integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="../../res/fancybox/jquery.fancybox.css" type="text/css" media="screen" />
  <script type="text/javascript" src="../../res/fancybox/jquery.fancybox.pack.js"></script>
  <script src="../../res/js/main.js" type="text/javascript"></script>
  <script type="text/javascript">
    $(document).ready(function () {
      $(".fancybox").fancybox();
      window.addEventListener('focus', checkFocus);
      window.addEventListener('blur', stopFocus); // Inactive
    });

    function checkFocus() { unmuteSketch()/* console.log("Is Focused") */ }
    function stopFocus() { muteSketch()/* console.log("Not Focused") */ }

    function muteSketch() {
      $('iframe').each(function () {
        $(this)[0].contentWindow.toggleMute(true);
      })
    }

    function unmuteSketch() {
      $('iframe').each(function () {
        $(this)[0].contentWindow.toggleMute(false);
      })
    }
  </script>
</head>

<body>
  <div class="container">

    <div class="wrapper">
      <p class="credit"><span>Atomic Language Machines</span></p><!--, by Daniel C Howe
      <p class="credit-media">Custom 14” NTSC monitors, bespoke software on embedded microcontrollers</p>-->

      <p class="main">
        The two types of reading machines below, <span style="color: rgb(170, 215, 247);">Radical of the Vertical Heart 忄</span>
        and <span style="color: rgb(245, 240, 182);">Automatype</span>, search
        their respective domains for bits of language most resembling their current state. The English
        machines move from word to word via the smallest number of possible mutations, each either a deletion, addition,
        or substitution. They attempt never to backtrack, yet their memory is unfortunately finite; they recognize
        tension and swerve at times toward surprise.
        <!--The Chinese machines (in simplified and traditional) move by shiting sub-character components (parts of characters) according to their rules of decompsition (see diagram below).
    Upon encountering "sensitive words" -- those that might be forbidden, for example, to cross the Great Firewall,
    they shift from simplified to traditional and vice-versa, in order to sidestep potential difficulties.-->
      </p>
      <blockquote>An entire mythology is stored within our language</blockquote>
      <p>
        The Chinese machines employ a new technique for assessing the proximity of characters and words via the analysis
        of their strokes, radicals, and semantic/phonemic sub-components. These machines tend to avoid repetition as
        well but demonstrate an affinity for top-to-bottom and right-to-left revolution. Certain sensitive words--those
        forbidden, for example, by the Great Firewall--jolt these readers into nearby domains (from traditional to
        simplified Chinese or vice versa), in order to circumvent potential difficulties.
        <!--It remains unclear as to the extent such violent shifts will adversely affect their long term viability.-->
      </p>
      <blockquote>Every explanation is after all a hypothesis</blockquote>
      <p>
        What are they searching for, and what does it mean that they are seem never to be satisfied with what they find?
        There is no adequate answer: every explanation is, after all, a hypothesis in itself. As a wise individual once
        said, 'Tell me how you are searching, and I will tell you what you are searching for.' I have only spoken here
        about the what and the how of this searching – the why must remain extracurricular.
      </p>

      <blockquote>We are asleep. Our life is a dream. But we wake up <br>sometimes, just long enough to know we are
        dreaming<sup> &Uarr;</sup></blockquote>

    </div>
    <div class="iframes">
      <iframe src="../autoch/index.html" style="margin-right:95px;"></iframe>
      <iframe src="../automatype/p5/index.html"></iframe>
    </div>

    <div class="wrapper transparent">
      <div class="figures">
        <img src="imgs/decompositions.png" class="decompositions" alt="">
        <p class="caption">The 12 primary decompositions for Chinese characters</p>

        <img src="imgs/code.png" class="code" alt="">
        <p class="caption">A new minimum edit distance for sinographic(Chinese) characters</p>
      </div>


      <div class="gallery">
        <div class="container">
          <a class="fancybox" rel="group" href="imgs/render1.jpg"
            title="Mapping 1000 iterations of an English machine (betweeness-centrality)"><img
              src="imgs/render1_preview.jpg" class="render"
              alt="Mapping 1000 iterations of an English machine (betweeness-centrality)"></a>
          <a class="fancybox" rel="group" href="imgs/render2.jpg"
            title="Mapping 1000 iterations of a simplified Chinese machine (betweeness-centrality)"><img
              src="imgs/render2_preview.jpg" class="render"
              alt="Mapping 1000 iterations of a simplified Chinese machine (betweeness-centrality)"></a>
          <a class="fancybox" rel="group" href="imgs/render3.jpg"
            title="Mapping 1000 iterations of a traditional Chinese machine (betweeness-centrality)"><img
              src="imgs/render3_preview.jpg" class="render"
              alt="Mapping 1000 iterations of a traditional Chinese machine (betweeness-centrality)"></a>
        </div>
      </div>
      <p class="caption">Mapping 1000 steps of various language machines (betweeness-centrality)</p>
    </div>

    <div class="wrapper">
      <p class="main">
        Technically-speaking we may characterise the entities above as <em>atomic language machines</em> (or ALMs), discrete
        computational entities with the potential to change the direction, intent, or magnitude of a literary vector. In
        general, ALMs can be defined as members of the simplest class of mechanisms able to realize linguistic
        advantage. Traditionally the term has referred to the three classical ALMs defined in early theoretical research
        associated with the ‘National Language Liberation Front’. The simplest of these were elementary devices
        consisting of a specific movement, often called a mechanism, which were to be combined with other devices and
        movements to form a new machine or machinic assembly. Thus even simple ALMs expressed modularity and could be
        considered as building blocks for more complex ones. Between simple atomic machines and complex assemblies,
        several intermediate classes have since been proposed, often termed compound-ALMs. This analytical view of ALMs
        as hierarchical modules decomposable into simple (or "atomic") machines first arose in the early 2020s as a
        post-AI-crisis expression of counter-vectoralist thought (see
        <a href="http://thereadersproject.org/">'the Readers Project'</a> for one prescient example), and now constitute
        a central part of applied linguistic praxis. For example, some of the simple ALMs on display here (the so-called
        construction, aggregation, and mutation engines) are still widely used in sinographic input methodologies
        (text-entry in Chinese, for example), predictive completion algorithms, and a host of other quotidian
        applications; a fact often obscured in the literature.
      </p>

      <p>See <a href="banner.html">project documentation</a> in (zoomable) high-resolution</p>
      <p>&nbsp;</p>
      <p class="caption" style="text-align:left;"><sup>&Uarr;</sup> All quotations by Ludwig Wittgenstein. Special
        thanks to Diane To. </p>


    </div>



    <div class="footer"></div>

</body>

</html>
